<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.27">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>SynthFair: A Semi-Synthetic Medical Imaging Dataset to Propel Research on Bias Detection &amp; Mitigation – SynthFair</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-ed96de9b727972fe78a7b5d16c58bf87.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-400fcced7b838d2a07658f7ad41044fb.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="floating nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">SynthFair</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link active" href="./index.html" aria-current="page"> 
<span class="menu-text">Home</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#synthetic-generation" id="toc-synthetic-generation" class="nav-link" data-scroll-target="#synthetic-generation">Synthetic Generation</a></li>
  <li><a href="#vae" id="toc-vae" class="nav-link" data-scroll-target="#vae">VAE</a>
  <ul class="collapse">
  <li><a href="#attribute-check" id="toc-attribute-check" class="nav-link" data-scroll-target="#attribute-check">Attribute Check</a></li>
  <li><a href="#image-quality" id="toc-image-quality" class="nav-link" data-scroll-target="#image-quality">Image Quality</a></li>
  </ul></li>
  <li><a href="#diffusion" id="toc-diffusion" class="nav-link" data-scroll-target="#diffusion">Diffusion</a>
  <ul class="collapse">
  <li><a href="#attribute-check-1" id="toc-attribute-check-1" class="nav-link" data-scroll-target="#attribute-check-1">Attribute Check</a></li>
  <li><a href="#image-quality-1" id="toc-image-quality-1" class="nav-link" data-scroll-target="#image-quality-1">Image Quality</a></li>
  </ul></li>
  <li><a href="#ddim" id="toc-ddim" class="nav-link" data-scroll-target="#ddim">DDIM</a>
  <ul class="collapse">
  <li><a href="#attribute-check-2" id="toc-attribute-check-2" class="nav-link" data-scroll-target="#attribute-check-2">Attribute Check</a></li>
  <li><a href="#image-quality-2" id="toc-image-quality-2" class="nav-link" data-scroll-target="#image-quality-2">Image Quality</a></li>
  </ul></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content column-page-right" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">SynthFair: A Semi-Synthetic Medical Imaging Dataset to Propel Research on Bias Detection &amp; Mitigation</h1>
</div>



<div class="quarto-title-meta column-page-right">

    
  
    
  </div>
  


</header>


<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>This research aims to create a high-quality semi-synthetic chest X-ray dataset designed to improve overall model performance and fairness. By generating controlled demographic variations of existing medical images, we seek to address representation imbalances and enable better bias detection and mitigation in medical imaging models.</p>
</section>
<section id="synthetic-generation" class="level2">
<h2 class="anchored" data-anchor-id="synthetic-generation">Synthetic Generation</h2>
<p>We first extracted demographic attributes (sex, race, and age) and clinical findings from each sample in the original dataset to construct a structured textual description. This text serves a dual purpose: it functions as a comprehensive image descriptor and acts as a conditioning prompt for generative models utilizing text encoders. Hereafter, we refer to this description as the “image prompt,” noting that it represents the semantic basis for generation even when not strictly used as a direct input for all architectures.</p>
<p>To generate specific variations, we applied interventions to the original prompts, modifying one attribute at a time. Adopting the notation of causal intervention, these shifts are denoted as <code>do(attribute=value)</code>. Examples of these shifts can be seen in Figure 1. This approach yields five distinct variation groups (three for race and two for sex). For each target variation, we aimed to generate 1,000 synthetic images. Crucially, the source images for these generations were sampled exclusively from groups disjoint from the target class, ensuring that all synthetic outputs represent a true modification of the original demographic attribute.</p>
<p>The fidelity of the generated images was assessed using three independent classifiers, one for each demographic attribute <a href="#references">[1]</a>. A synthetic image is considered successfully generated only if it satisfies two conditions: (1) the target intervention is correctly predicted, and (2) all non-intervened attributes remain consistent with the original image. If any attribute fails this check, the generation is retried up to three times. If the prediction fails after three attempts, the prompt is discarded. This validation process is referred to as the <em>Attribute Check</em> module. Consequently, the final dataset size depends on the acceptance rate of this validation step, with a theoretical maximum of 5,000 images (5 groups × 1k samples). We evaluated this pipeline across three generative architectures: a Variational Autoencoder (VAE) producing counterfactuals <a href="#references">[2]</a>, a diffusion model fine-tuned on MIMIC-CXR (Roentgenv2 <a href="#references">[3]</a>), and a DDIM inversion scheme applied to the counterfactual version of the latter <a href="#references">[4]</a>. An overview of the pipeline used in this work can be seen below.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./imgs/pipeline_original.png" class="img-fluid figure-img"></p>
<figcaption>Pipeline for synthetic image generation. From a CXR dataset, prompts are built based on each sample’s metadata. Then, different variations of these prompts are constructed to generate a new synthetic dataset using a generative model. For a generated image to be part of the new dataset, it must successfully pass the attribute check.</figcaption>
</figure>
</div>
</section>
<section id="vae" class="level2">
<h2 class="anchored" data-anchor-id="vae">VAE</h2>
<p>To generate high-fidelity counterfactual images, we based the generation on Deep Structural Causal Models (DSCM). Unlike traditional approaches that only learn the statistical distribution of data, this model explicitly defines the causal relationships between demographic attributes (causes) and the medical image (effect) through a Structural Causal Model (SCM).</p>
<p>The core architecture for the image component is a Hierarchical Variational Autoencoder (HVAE) (Figure 2). We selected this architecture for its ability to compress the image into a probabilistic latent space, allowing us to disentangle the semantic attributes from the specific visual details of the patient.</p>
<p>To generate a variation of an image while preserving the patient’s unique identity, we utilize Pearl’s three-step ladder of causation (Figure 3). This process ensures that only the target attribute is modified, while the rest of the image’s content remains consistent. The process is defined as follows:</p>
<ol type="1">
<li><p><strong>Abduction (Inferring the specific context)</strong>: The model takes the original image <span class="math inline">X</span> and passes it through the VAE encoder. The goal is to infer the posterior distribution of the <em>exogenous noise</em> <span class="math inline">u</span>. In simpler terms, the model captures all the unique characteristics of the patient (such as bone structure, posture, or specific anatomical details) that are not explained by the demographic attributes. This noise represents the patient’s visual identity.</p></li>
<li><p><strong>Action (The intervention)</strong>: We perform a causal intervention on the graph, denoted as <span class="math inline">do(A=a')</span>. This step mathematically breaks the dependence on the original attribute, allowing us to simulate a hypothetical scenario.</p></li>
<li><p><strong>Prediction (Generating the counterfactual)</strong>: Finally, the VAE decoder generates a new image <span class="math inline">X^*</span> using the intervention defined in the <em>action step</em>, but combined with the original exogenous noise <span class="math inline">u</span> recovered during the <em>abduction step</em>. By reusing the original noise, the model ensures that the resulting image retains the specific anatomy and details of the original patient, changing only the visual features causally linked to the intervened attribute.</p></li>
</ol>
<p>An example of different counterfactuals generated from the same original image can be seen below.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./imgs/chexpert_vae_comparison.png" class="img-fluid figure-img"></p>
<figcaption>Figure 1. Different prompt variations using the VAE generative model. All images passed the attribute check successfully.</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./imgs/vae_1.png" class="img-fluid figure-img"></p>
<figcaption>Figure 2. Hierarchical structure of the Variational Autoencoder (HVAE) used for image generation. The architecture consists of multiple hierarchical latent levels (<span class="math inline">h_1, h_2, h_3</span>) initialized from <span class="math inline">h_{\text{init}}</span>, where each level generates both demographic attribute representations (<span class="math inline">\text{pa}_i</span>) and image-specific latent variables (<span class="math inline">z_i</span>). This hierarchical decomposition flows downward through the red path to generate the final image <span class="math inline">x</span>, allowing the model to disentangle demographic attributes from patient-specific visual characteristics at different levels of abstraction.</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./imgs/vae_2.png" class="img-fluid figure-img"></p>
<figcaption>Figure 3. Structural Causal Model (SCM) for counterfactual generation showing the three-step intervention process. The diagram illustrates the causal relationships with exogenous variables (<span class="math inline">U_{\text{pa}}, U_z, U_x</span>) that capture unexplained variation. <span class="math inline">\text{pa}_x \rightarrow x</span> path shows the original image generation, while <span class="math inline">\tilde{\text{pa}}_x \rightarrow \tilde{x}</span> path shows the counterfactual generation. The latent variables <span class="math inline">z_{1:L}</span> and <span class="math inline">\tilde{z}_{1:L}</span> represent the hierarchical latent space. During counterfactual generation, the intervention is applied to the demographic attribute (<span class="math inline">\text{pa}_x \rightarrow \tilde{\text{pa}}_x</span>), while the exogenous noise variables (<span class="math inline">U</span>) are reused to preserve patient-specific characteristics, ensuring that only the target demographic attribute changes while maintaining anatomical consistency.</figcaption>
</figure>
</div>
<p>The model generates images at a low 192×192 resolution, while the demographic classifiers used for validation work at higher resolutions (299×299 and 320×320). This resolution mismatch means that the generated images from the HVAE model must undergo an upscaling process before the attribute check. To analyze whether these classifiers are sensitive to this resolution change, and to determine whether upscaling could potentially degrade image quality or classifier performance, we tested different types of upscaling before the attribute check, modifying the original pipeline as shown below.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./imgs/pipeline_modified.png" class="img-fluid figure-img"></p>
<figcaption>Modified pipeline that includes an upscaling of the generated images before going through the attribute check. This helps to evaluate how sensitive to image resolution the attribute check is.</figcaption>
</figure>
</div>
<p>First, we applied simple upscaling using LANCZOS interpolation to 512×512 resolution (the same resolution that the Roentgenv2 diffusion model uses), followed by a 3× super-resolution upscaling to 576×576 using the EDSR model <a href="#references">[5]</a>.</p>
<section id="attribute-check" class="level3">
<h3 class="anchored" data-anchor-id="attribute-check">Attribute Check</h3>
<p>As noted before, the attribute check consists of three independent models trained for sex and race classification and age regression. In this module of the pipeline, a generated image goes through each model obtaining a prediction for the three attributes. One of these attributes is the one that is being shifted (denoted as <em>target group</em>). If all three attributes are predicted correctly on the synthetic image then it will be part of the new dataset and it will count toward the percentage passed attribute check. In Figure 4 the attribute check results for each target group are shown for the VAE model and the different types of upscaling and for images generated from CheXpert <a href="#references">[6]</a> and MIMIC-CXR <a href="#references">[7]</a> metadata.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./imgs/all_vae_check.png" class="img-fluid figure-img"></p>
<figcaption>Figure 4. Attribute check results for the three versions of the VAE model based on CheXpert (left) and MIMIC-CXR (right). Results show the percentage of generated images that successfully pass the validation for each demographic attribute. Three configurations are compared: (1) Original low-resolution VAE (192×192), (2) upscaling using LANCZOS interpolation (512×512), and (3) 3× super-resolution upscaling using EDSR (576×576). Higher bars indicate better attribute preservation and transformation accuracy.</figcaption>
</figure>
</div>
</section>
<section id="image-quality" class="level3">
<h3 class="anchored" data-anchor-id="image-quality">Image Quality</h3>
<p>Even though the attribute check validation is a necessary process to ensure that the generated images follow the desired attributes, it does not clarify the quality of those generated images. For this reason, we computed the Fréchet Distance <a href="#references">[8]</a> for each group using Inceptionv3 <a href="#references">[9]</a> and RadDINO <a href="#references">[10]</a> latent space representations (Figure 5). This metric provides a more realistic measure of how similar the synthetic distributions are to the original distribution. The distance in the Inceptionv3 embeddings represents general characteristics of the images, as this model was trained on general images. On the other hand, the distance in the RadDINO embeddings reflects the quality of the images in terms of medical imaging characteristics.</p>
<p><img src="./imgs/chexpert_all_vae_fid.png" class="img-fluid"></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./imgs/roentgen_all_vae_fid.png" class="img-fluid figure-img"></p>
<figcaption>Figure 5. Fréchet distance comparison between the three VAE models for counterfactual generation using Inceptionv3 (left) and RadDINO (right) embeddings. Results are shown for CheXpert (top) and MIMIC-CXR (bottom) based generations. Lower values indicate better alignment with the original data distribution.</figcaption>
</figure>
</div>
<p>Based on these results, we concluded that upscaling the images is worthwhile and demonstrated that the VAE model produces high-quality images that can be upscaled without degrading their quality. The super-resolution approach using EDSR particularly improves the attribute check success rate while maintaining strong distributional alignment with the original dataset. The UMAP visualizations (Figure 6) confirm that VAE-generated images occupy similar regions in the latent space as real images, indicating that the counterfactuals preserve realistic medical imaging characteristics.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./imgs/chexpert_all_vae_density.png" class="img-fluid figure-img"></p>
<figcaption>Figure 6. UMAP projection visualizations comparing the latent space distributions of original CheXpert images (blue) versus VAE-generated images (red) across three different embedding spaces: sex prediction model from attribute check (top), Inceptionv3 (middle), and RadDINO (bottom). Closer overlap between distributions indicates better generation quality and realism. Each plot shows how well the synthetic images capture the statistical properties of the original dataset in different feature representations.</figcaption>
</figure>
</div>
</section>
</section>
<section id="diffusion" class="level2">
<h2 class="anchored" data-anchor-id="diffusion">Diffusion</h2>
<p>Differing from the VAE model where we applied direct counterfactuals to the original image, in this approach we focus on generating images by random sampling from noise. For this, we use the Roentgenv2 model that was trained specifically for medical image generation by fine-tuning Stable Diffusion with samples from MIMIC-CXR. The architecture of this model can be seen in Figure 7.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./imgs/roentgen_architecture.png" class="img-fluid figure-img"></p>
<figcaption>Figure 7. Architecture of the Roentgenv2 model, a fine-tuned version of Stable Diffusion specifically adapted for chest X-ray generation using the MIMIC-CXR dataset. The model uses text conditioning to guide the diffusion process from random noise to realistic medical images.</figcaption>
</figure>
</div>
<p>Every image is generated by random sampling from noise, with the generation process primarily driven by the text prompt. This means that only the metadata from the original dataset is used, not the image itself. Multiple generations with the same prompt can produce different final states, resulting in diverse new images. As can be seen in the synthetic examples in Figure 8, the generated images are completely different from the original image since the original is not used during the generative process, resulting in very significant visual differences.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./imgs/chexpert_dif_comparison.png" class="img-fluid figure-img"></p>
<figcaption>Figure 8. Comparison of different prompt variations using the Roentgenv2 diffusion model with random sampling. Unlike the VAE counterfactuals, these images are generated entirely from noise and show significant visual differences from the original images, as the generation process does not preserve patient-specific anatomical features.</figcaption>
</figure>
</div>
<section id="attribute-check-1" class="level3">
<h3 class="anchored" data-anchor-id="attribute-check-1">Attribute Check</h3>
<p>The attribute check validation results show that the VAE-generated images have an overall higher success rate (Figure 9). The most significant discrepancy comes from the <em>Asian</em> target group, where the VAE struggles more than the diffusion model. It is worth noticing that even though the Roentgenv2 model was trained using the MIMIC-CXR images, it has a lower success rate on this dataset than the VAE model. This suggests that the latter model has better generalization for image generation.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./imgs/vae_dif_check.png" class="img-fluid figure-img"></p>
<figcaption>Figure 9. Attribute check comparison between VAE counterfactual approach and diffusion random sampling approach for both CheXpert and MIMIC-CXR based generations. The bars show the percentage of generated images that successfully pass the demographic attribute validation for each target group.</figcaption>
</figure>
</div>
</section>
<section id="image-quality-1" class="level3">
<h3 class="anchored" data-anchor-id="image-quality-1">Image Quality</h3>
<p>By looking at the Fréchet distances (Figure 10), we further understand the differences between both generative models. The VAE model is far more accurate in terms of these distances, generating more reliable and realistic images than the Roentgenv2 model. These high distance values presented by the diffusion model can be explained by the possibility of hallucinations that the diffusion process has, as can be shown in the examples in Figure 11. These are images that were generated by the Roentgenv2 model and, more concerning, successfully passed the attribute check validation.</p>
<p><img src="./imgs/chexpert_vae_dif_fid.png" class="img-fluid"></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./imgs/roentgen_vae_dif_fid.png" class="img-fluid figure-img"></p>
<figcaption>Figure 10. Fréchet distance comparison between VAE counterfactual generation and diffusion random sampling using Inceptionv3 (left) and RadDINO (right) embeddings. Results are shown for CheXpert (top) and MIMIC-CXR (bottom) based generations. Lower values indicate better alignment with the original data distribution.</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./imgs/roentgen_hallucinations.png" class="img-fluid figure-img"></p>
<figcaption>Figure 11. Examples of images generated by Roentgenv2 model showing hallucinations that can appear using these types of models. All these images successfully passed the attribute check validation.</figcaption>
</figure>
</div>
<p>The UMAP projections of the diffusion-generated images show how limited the representation can be compared to the VAE-generated images (Figure 12).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./imgs/chexpert_vae_dif_density.png" class="img-fluid figure-img"></p>
<figcaption>Figure 12. UMAP projection visualizations comparing the latent space distributions of original CheXpert images (blue) versus VAE- and diffusion- generated images (red) across three different embedding spaces: sex prediction model from attribute check (top), Inceptionv3 (middle), and RadDINO (bottom). Closer overlap between distributions indicates better generation quality and realism. Each plot shows how well the synthetic images capture the statistical properties of the original dataset in different feature representations.</figcaption>
</figure>
</div>
</section>
</section>
<section id="ddim" class="level2">
<h2 class="anchored" data-anchor-id="ddim">DDIM</h2>
<p>While the Roentgenv2 diffusion model generates images through random sampling from noise, losing the connection to the original image structure, the DDIM (Denoising Diffusion Implicit Models) inversion approach offers a middle ground that combines the high visual fidelity of the diffusion model with the counterfactual nature of the VAE approach. This method enables us to generate variations that preserve more structural information from the original image while still leveraging the powerful generative capabilities of the Roentgenv2 model.</p>
<p>The DDIM inversion process works in two main stages:</p>
<ol type="1">
<li><p><strong>Inversion (Encoding to latent noise)</strong>: Starting from a real image <span class="math inline">X</span> from the original dataset, we reverse the diffusion process to obtain a latent noise representation <span class="math inline">z_0</span>. Unlike standard diffusion sampling that starts from pure random noise, DDIM inversion uses a deterministic process to trace back the specific noise configuration that would have generated an image similar to the input. This step essentially <em>encodes</em> the structural and anatomical information of the original patient into the latent space. The inversion is performed using the original image prompt (with the unmodified demographic attributes) to guide the reverse process.</p></li>
<li><p><strong>Conditional generation (Decoding with intervention)</strong>: Once we have the inverted latent representation <span class="math inline">z_0</span>, we perform the forward diffusion sampling process, but now using a modified prompt with the intervened attribute, denoted as <code>do(attribute=value')</code>. The key insight is that by starting from the inverted noise rather than random noise, the generation process preserves the overall anatomical structure and specific patient characteristics from the original image, while the modified prompt guides the model to alter only the visual features associated with the target demographic attribute.</p></li>
</ol>
<p>This approach offers several advantages over pure random sampling: the generated counterfactuals maintain stronger anatomical coherence with the source image, preserve patient-specific details unrelated to the intervened attribute, and produce more consistent counterfactuals across different intervention attempts. However, the quality and faithfulness of the counterfactuals depend critically on two hyperparameters: the number of inversion steps and the classifier-free guidance scale, which controls the strength of the text conditioning during generation.</p>
<p>We evaluated different configurations of these parameters to find the optimal balance between preserving the original image structure and successfully implementing the demographic intervention. The DDIM approach produces counterfactuals that are visually closer to the original image compared to random sampling, while still achieving successful attribute transformations. As shown in Figure 13, different guidance scales (7.5 vs 15.5) produce different levels of attribute modification, with higher values creating more pronounced changes.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./imgs/chexpert_ddim_comparison.png" class="img-fluid figure-img"></p>
<figcaption>Figure 13. DDIM inversion counterfactual examples with 50 inversion steps (100% inversion) and classifier-free guidance scale of 7.5 and 15.5. The generated images show some preservation of anatomical structure from the original image while successfully modifying the target demographic attribute. Higher guidance scale produces more pronounced attribute changes.</figcaption>
</figure>
</div>
<section id="attribute-check-2" class="level3">
<h3 class="anchored" data-anchor-id="attribute-check-2">Attribute Check</h3>
<p>The results in Figure 14 show that the DDIM process has a more equal success rate across the different groups, but overall is lower than for VAE and Diffusion models. It is worth noticing that it was the best-performing model on the <em>Asian</em> target group.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./imgs/vae_dif_ddim_check.png" class="img-fluid figure-img"></p>
<figcaption>Figure 14. Comprehensive attribute check comparison across all three generative approaches: VAE counterfactuals, diffusion random sampling, and DDIM inversion. Results are shown for both CheXpert (left) and MIMIC-CXR (right) based generations. The bars represent the percentage of generated images that successfully pass demographic attribute validation.</figcaption>
</figure>
</div>
</section>
<section id="image-quality-2" class="level3">
<h3 class="anchored" data-anchor-id="image-quality-2">Image Quality</h3>
<p>The Fréchet distance results (Figure 15) demonstrate that DDIM achieves intermediate performance between VAE and pure diffusion sampling. While not matching the VAE’s distributional alignment, DDIM substantially improves upon random diffusion sampling by leveraging the original image structure through inversion. The UMAP visualizations (Figure 16) further illustrate how DDIM-generated images occupy an intermediate position in the latent space, showing better overlap with real images than diffusion sampling but slightly less than VAE counterfactuals.</p>
<p><img src="./imgs/chexpert_vae_dif_ddim_fid.png" class="img-fluid"></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./imgs/roentgen_vae_dif_ddim_fid.png" class="img-fluid figure-img"></p>
<figcaption>Figure 15. Fréchet distance comparison across all three generative approaches for MIMIC-CXR based generations using Inceptionv3 (left) and RadDINO (right) embeddings. Results show consistent trends with CheXpert-based generations.</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./imgs/chexpert_vae_dif_ddim_density.png" class="img-fluid figure-img"></p>
<figcaption>Figure 16. UMAP projection visualizations comparing the latent space distributions of original CheXpert images (blue) versus all three generative approaches (red) across three different embedding spaces: sex prediction model from attribute check (top), Inceptionv3 (middle), and RadDINO (bottom). Closer overlap between distributions indicates better generation quality and realism. Each plot shows how well the synthetic images capture the statistical properties of the original dataset in different feature representations.</figcaption>
</figure>
</div>
</section>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<p>[1] Cohen, Joseph Paul, et al.&nbsp;“TorchXRayVision: A library of chest X-ray datasets and models.” arXiv, 2021, <a href="https://arxiv.org/abs/2111.00595">https://arxiv.org/abs/2111.00595</a>.</p>
<p>[2] Ribeiro, Fabio De Sousa, et al.&nbsp;“High Fidelity Image Counterfactuals with Probabilistic Causal Models.” arXiv, 2023, <a href="https://arxiv.org/abs/2306.15764">https://arxiv.org/abs/2306.15764</a>.</p>
<p>[3] Moroianu, Stefania L., et al.&nbsp;“Improving Performance, Robustness, and Fairness of Radiographic AI Models with Finely-Controllable Synthetic Data.” arXiv, 2025, <a href="https://arxiv.org/abs/2508.16783">https://arxiv.org/abs/2508.16783</a>.</p>
<p>[4] Song, Jiaming, et al.&nbsp;“Denoising Diffusion Implicit Models.” arXiv, 2022, <a href="https://arxiv.org/abs/2010.02502">https://arxiv.org/abs/2010.02502</a>.</p>
<p>[5] Lim, Bee, et al.&nbsp;“Enhanced Deep Residual Networks for Single Image Super-Resolution.” arXiv, 2017, <a href="https://arxiv.org/abs/1707.02921">https://arxiv.org/abs/1707.02921</a>.</p>
<p>[6] Irvin, Jeremy, et al.&nbsp;“CheXpert: A Large Chest Radiograph Dataset with Uncertainty Labels and Expert Comparison.” arXiv, 2019, <a href="https://arxiv.org/abs/1901.07031">https://arxiv.org/abs/1901.07031</a>.</p>
<p>[7] Johnson, A. E. W., et al.&nbsp;“MIMIC-CXR, a De-identified Publicly Available Database of Chest Radiographs with Free-text Reports.” Scientific Data, vol.&nbsp;6, 2019, <a href="https://doi.org/10.1038/s41597-019-0322-0">https://doi.org/10.1038/s41597-019-0322-0</a>.</p>
<p>[8] Heusel, Martin, et al.&nbsp;“GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium.” arXiv, 2018, <a href="https://arxiv.org/abs/1706.08500">https://arxiv.org/abs/1706.08500</a>.</p>
<p>[9] Szegedy, Christian, et al.&nbsp;“Rethinking the Inception Architecture for Computer Vision.” arXiv, 2015, <a href="https://arxiv.org/abs/1512.00567">https://arxiv.org/abs/1512.00567</a>.</p>
<p>[10] Pérez-García, F., et al.&nbsp;“Exploring Scalable Medical Image Encoders Beyond Text Supervision.” Nature Machine Intelligence, vol.&nbsp;7, 2025, pp.&nbsp;119-130, <a href="https://doi.org/10.1038/s42256-024-00965-w">https://doi.org/10.1038/s42256-024-00965-w</a>.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/eclaucich-sinc\.github\.io\/SynthFair-webpage\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




<script src="site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>